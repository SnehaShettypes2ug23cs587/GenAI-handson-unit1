{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td4sZlbiKX0U"
      },
      "source": [
        "# Unit 1 Assignment: The Model Benchmark Challenge\n",
        "\n",
        "**Objective:** Evaluate architectural differences between BERT, RoBERTa, and BART by testing them on tasks they may not be designed for.\n",
        "\n",
        "**Models to Test:**\n",
        "1. BERT (`bert-base-uncased`) - Encoder-only\n",
        "2. RoBERTa (`roberta-base`) - Encoder-only (optimized)\n",
        "3. BART (`facebook/bart-base`) - Encoder-Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYoCcwEEKX0V"
      },
      "source": [
        "## Setup: Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "npz51DNuKX0W"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlZkSfR1KX0W",
        "outputId": "062f52f1-a199-45da-a119-d6d129c8b20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj0Uxz_cKX0W"
      },
      "source": [
        "---\n",
        "## Experiment 1: Text Generation\n",
        "\n",
        "**Task:** Generate text using the prompt: `\"The future of Artificial Intelligence is\"`\n",
        "\n",
        "**Hypothesis:** BERT and RoBERTa will fail because they're encoder-only models designed to understand text, not generate it. BART might do better since it's an encoder-decoder model, but probably won't be great without proper training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-gwb1DdKX0W",
        "outputId": "6ece3595-f1f3-4305-e48f-e126b87940a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'The future of Artificial Intelligence is'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "print(f\"Prompt: '{prompt}'\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwVzjd1uKX0X"
      },
      "source": [
        "### Test 1.1: BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwNrohXXKX0Y",
        "outputId": "57408126-d4b1-44e7-eda9-f026b17571e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Testing BERT for Text Generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n Testing BERT for Text Generation...\")\n",
        "try:\n",
        "    bert_generator = pipeline('text-generation', model='bert-base-uncased')\n",
        "    result = bert_generator(prompt, max_length=20, num_return_sequences=1)\n",
        "    print(f\"Result: {result[0]['generated_text']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D9yxQSUKX0Z"
      },
      "source": [
        "### Test 1.2: RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2K54-cWKX0Z",
        "outputId": "2be19d2e-1906-48d5-b695-cacaa88ff796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing RoBERTa for Text Generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: The future of Artificial Intelligence is\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTesting RoBERTa for Text Generation...\")\n",
        "try:\n",
        "    roberta_generator = pipeline('text-generation', model='roberta-base')\n",
        "    result = roberta_generator(prompt, max_length=20, num_return_sequences=1)\n",
        "    print(f\"Result: {result[0]['generated_text']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf5irtuaKX0a"
      },
      "source": [
        "### Test 1.3: BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVGLIWnJKX0a",
        "outputId": "533f7a6d-e56f-4017-c23f-e6ac9d7b46bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing BART for Text Generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: The future of Artificial Intelligence is mythology ampUBAntiAntiAntiExt83jandroprisonprisonprison UNESCOprisonExtExt doctrinesListprisonprison rallying definitelyExt Deputy Deputy extracted extracted insultsjerprisonExtCVprisonExtprison concedesprisonprison ruininglaws Motorsport Motorsport necessprisonCSExtExt ampprisonprison taxedprison necessExtExtprison extracted extracted accomplishedprison doctrinesExt accomplished necess necessthatprison doctrines padded necesshodExt AsExt Deputyhodprisonprison doctrinesprisonGROUP glorious As nervprison AsExt Customs conjectureprison extracted Customs Ancients CustomsERSON extracted extracted extracted conjectureision Conc distribution necess necess fractures Customs CustomsAnti Customs Customs sock doctrines vulnerability extracted extracted Customs Customs Customs accomplished Customseneg Customs Customsclair Customs Ancientseneg oslaws vulnerability vulnerability As Customs necessenegERSON extractedclairprison CustomsAnti eventual vulnerabilityUB CustomsERSONERSON Deputy As distribution necess killing permitted permittedision CustomsERSONclair As Ancients Customsclairclairfights vulnerabilityUB As necess vulnerability nerv necessERSON distribution Asbe extracted distribution As bounty nerv extractedERSONhod extracted vulnerabilityERSON nerv nerv nerv vulnerabilityERSON extracted vulnerabilitybe Cisco eventual Gim eventual Gim nerv bounty nominatingERSONhod nerv Ancients Customs vulnerabilityERSONduino necess permittedbe nerv Star extracted borne vulnerability vulnerabilityprocessing extracted vulnerability Star extracted StarERSONERSONeneg As Ancients Illegal borne extracted accomplishedbeudd extracted eventual accomplishedERSONERSONERSON padded handlerbe eventual extracted nerv accomplished nerv Star eventual accomplished eventual accomplished\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTesting BART for Text Generation...\")\n",
        "try:\n",
        "    bart_generator = pipeline('text-generation', model='facebook/bart-base')\n",
        "    result = bart_generator(prompt, max_length=20, num_return_sequences=1)\n",
        "    print(f\"Result: {result[0]['generated_text']}\")\n",
        "except Exception as e:\n",
        "    print(f\" Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_dEs-AGKX0b"
      },
      "source": [
        "---\n",
        "## Experiment 2: Masked Language Modeling (Fill-Mask)\n",
        "\n",
        "**Task:** Predict the missing word in: `\"The goal of Generative AI is to [MASK] new content.\"`\n",
        "\n",
        "**Hypothesis:** BERT and RoBERTa should crush this task since they were literally trained on predicting masked words. BART might work but won't be as good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYYoPBpxKX0b"
      },
      "source": [
        "### Test 2.1: BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhsudWKIKX0b",
        "outputId": "6d37c5d5-4ee5-4d74-d02c-5f4a364ffd7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing BERT for Fill-Mask...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Predictions:\n",
            "   1. create (score: 0.5397)\n",
            "   2. generate (score: 0.1558)\n",
            "   3. produce (score: 0.0541)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTesting BERT for Fill-Mask...\")\n",
        "try:\n",
        "    bert_mask = pipeline('fill-mask', model='bert-base-uncased')\n",
        "    sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "    result = bert_mask(sentence)\n",
        "    print(f\"Top 3 Predictions:\")\n",
        "    for i, pred in enumerate(result[:3], 1):\n",
        "        print(f\"   {i}. {pred['token_str']} (score: {pred['score']:.4f})\")\n",
        "except Exception as e:\n",
        "    print(f\" Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMyUeh1uKX0b"
      },
      "source": [
        "### Test 2.2: RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdnfCeU7KX0b",
        "outputId": "4bd6419c-f628-4df7-8fe2-1797e18bc881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing RoBERTa for Fill-Mask...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Predictions:\n",
            "   1.  generate (score: 0.3711)\n",
            "   2.  create (score: 0.3677)\n",
            "   3.  discover (score: 0.0835)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTesting RoBERTa for Fill-Mask...\")\n",
        "try:\n",
        "    roberta_mask = pipeline('fill-mask', model='roberta-base')\n",
        "    # RoBERTa uses <mask> instead of [MASK]\n",
        "    sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
        "    result = roberta_mask(sentence)\n",
        "    print(f\"Top 3 Predictions:\")\n",
        "    for i, pred in enumerate(result[:3], 1):\n",
        "        print(f\"   {i}. {pred['token_str']} (score: {pred['score']:.4f})\")\n",
        "except Exception as e:\n",
        "    print(f\" Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-4g6cfOKX0b"
      },
      "source": [
        "### Test 2.3: BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-4ccbnPKX0b",
        "outputId": "8aa65ce3-9916-47af-9258-37e518a57972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing BART for Fill-Mask...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Predictions:\n",
            "   1.  create (score: 0.0746)\n",
            "   2.  help (score: 0.0657)\n",
            "   3.  provide (score: 0.0609)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTesting BART for Fill-Mask...\")\n",
        "try:\n",
        "    bart_mask = pipeline('fill-mask', model='facebook/bart-base')\n",
        "    # BART uses <mask> token\n",
        "    sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
        "    result = bart_mask(sentence)\n",
        "    print(f\"Top 3 Predictions:\")\n",
        "    for i, pred in enumerate(result[:3], 1):\n",
        "        print(f\"   {i}. {pred['token_str']} (score: {pred['score']:.4f})\")\n",
        "except Exception as e:\n",
        "    print(f\" Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLmMjyxJKX0b"
      },
      "source": [
        "---\n",
        "## Experiment 3: Question Answering\n",
        "\n",
        "**Task:** Answer the question `\"What are the risks?\"` based on the context:\n",
        "`\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"`\n",
        "\n",
        "**Hypothesis:** All three base models will probably struggle since none of them are fine-tuned for QA tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v43xsaT6KX0b",
        "outputId": "f32f72f1-ad2e-4a49-df64-447d51dcc472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\n",
            "Question: What are the risks?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "print(f\"Context: {context}\")\n",
        "print(f\"Question: {question}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwPWjRC7KX0b"
      },
      "source": [
        "### Test 3.1: BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTPzEmuFKX0b",
        "outputId": "842050bd-04e8-4e92-8d2f-50f35d7d8e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Testing BERT for Question Answering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: deepfakes.\n",
            "Confidence: 0.0112\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n Testing BERT for Question Answering...\")\n",
        "try:\n",
        "    bert_qa = pipeline('question-answering', model='bert-base-uncased')\n",
        "    result = bert_qa(question=question, context=context)\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "    print(f\"Confidence: {result['score']:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\" Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz-UNuFZKX0b"
      },
      "source": [
        "### Test 3.2: RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFo20N0SKX0b",
        "outputId": "167c175a-6737-4a0a-dfd4-5c12be18b356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing RoBERTa for Question Answering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: deepfakes\n",
            "Confidence: 0.0119\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTesting RoBERTa for Question Answering...\")\n",
        "try:\n",
        "    roberta_qa = pipeline('question-answering', model='roberta-base')\n",
        "    result = roberta_qa(question=question, context=context)\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "    print(f\"Confidence: {result['score']:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OgwufVBKX0c"
      },
      "source": [
        "### Test 3.3: BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo3U-a2xKX0c",
        "outputId": "f01da231-6d7f-4dba-f5bc-7e238ac92570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Testing BART for Question Answering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Answer: deepfakes.\n",
            "   Confidence: 0.0286\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n Testing BART for Question Answering...\")\n",
        "try:\n",
        "    bart_qa = pipeline('question-answering', model='facebook/bart-base')\n",
        "    result = bart_qa(question=question, context=context)\n",
        "    print(f\" Answer: {result['answer']}\")\n",
        "    print(f\"   Confidence: {result['score']:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error: {str(e)}\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edelr00aKX0c"
      },
      "source": [
        "---\n",
        "## Summary & Observation Table\n",
        "\n",
        "Based on the experiments above, fill in your observations below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPHomjiKKX0c"
      },
      "source": [
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|------|-------|----------------------------------|---------------------------------------|---------------------------------------------|\n",
        "| **Generation** | BERT | Failure | Generated repeated dots/periods instead of coherent text | BERT is an Encoder-only model trained for understanding (MLM), not for generating sequential text |\n",
        "| | RoBERTa | Failure | Simply repeated the input prompt without generating new text | RoBERTa is an Encoder-only model (optimized BERT), not designed for sequential text generation |\n",
        "| | BART | Failure | Generated completely incoherent text with random repetitive words (roach, Bella, distributors, juice, etc.) | BART is designed for seq2seq tasks, not causal generation. The causal LM head wasn't properly initialized, resulting in nonsense output |\n",
        "| **Fill-Mask** | BERT | Success | Predicted 'create' (53.97%), 'generate' (15.58%), 'produce' (5.41%) - all semantically correct | BERT was trained specifically on Masked Language Modeling (MLM), making this its core strength |\n",
        "| | RoBERTa | Success | Predicted 'generate' (37.11%), 'create' (36.77%), 'discover' (8.35%) - all semantically correct with more balanced confidence | RoBERTa is an optimized BERT with improved MLM training, resulting in better-calibrated predictions |\n",
        "| | BART | Partial Success | Predicted 'create' (7.46%), 'help' (6.57%), 'provide' (6.09%) - much lower confidence than BERT/RoBERTa; less optimal predictions | BART is trained with denoising but optimized for seq2seq tasks, not pure MLM; less specialized for fill-mask |\n",
        "| **QA** | BERT | Failure | Extracted only 'deepfakes.' (1.12%) with very low confidence; incomplete answer missing 'hallucinations' and 'bias' | Base BERT not fine-tuned for QA; randomly initialized QA head lacks specialized training for complete answer extraction |\n",
        "| | RoBERTa | Failure | Extracted only 'deepfakes' (1.19%) with very low confidence; incomplete answer, same as BERT | Base RoBERTa not fine-tuned for QA; despite better representations, randomly initialized QA head performs similarly poorly |\n",
        "| | BART | Failure | Extracted only 'deepfakes.' (2.86%) with very low confidence; incomplete answer but highest confidence among all three models | BART designed for seq2seq generation, not extractive QA; randomly initialized QA head still performs poorly despite higher relative confidence |"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qk4gUsMhUxzj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
